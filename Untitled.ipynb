{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Scientific Packages into Python Kernel\n",
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "import re \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set a user-agent in your header so you aren't flagged by the browser when making an HTTP request\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "           Chrome/39.0.2171.95 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Link url from caselaw which has a repository of Supreme Court ruling opinions and assign to variable\n",
    "root_url = \"http://caselaw.findlaw.com/court/us-supreme-court/years/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign a variable to house an array of supreme court documents listed within the years you'd like to explore\n",
    "years = [root_url + str(year) for year in range(1760,2018)]\n",
    "\n",
    "# Define a method that executes your url request and returns the data (HTML or XML) as an Object \n",
    "def Beautiful_soup_grabber(link):\n",
    "    \n",
    "    response = requests.get(link, headers = headers) #optional add timeout (seconds) keeps requests from running indefinitely \n",
    "    \n",
    "    return BeautifulSoup(response.text, \"lxml\") #Returns BeautifulSoup object, which represents the document as a nested data structure\n",
    "\n",
    "\n",
    "# Define a method which calls the above method for each year within the range you've requested and convert result object into table\n",
    "def year_getter(years):\n",
    "    \n",
    "    y = {}\n",
    "    for year in years:\n",
    "        soup = Beautiful_soup_grabber(year)\n",
    "        souplist = soup.findAll(\"a\")\n",
    "        \n",
    "        #use regular expressions to \n",
    "        for i in souplist:\n",
    "            if re.search(\"us-supreme-court\", str(i)) and not re.search(\"years\", str(i)) and not re.search(\"/court/\", str(i)):\n",
    "                b = i[\"href\"]\n",
    "                y[b] = [re.sub(\"[^0-9]\", \"\", b.split(\"/\")[-1])]\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(y).transpose().reset_index() #converts results to data frame table using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = year_getter(years) #call the above function and assign it to a shorthand varible (this will take several minutes to execute)\n",
    "\n",
    "df.columns = [\"case_url\", \"docket\"] #assign column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_url</th>\n",
       "      <th>docket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://caselaw.findlaw.com/us-supreme-court/05...</td>\n",
       "      <td>051101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://caselaw.findlaw.com/us-supreme-court/06...</td>\n",
       "      <td>0611951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://caselaw.findlaw.com/us-supreme-court/06...</td>\n",
       "      <td>06263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://caselaw.findlaw.com/us-supreme-court/06...</td>\n",
       "      <td>065590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://caselaw.findlaw.com/us-supreme-court/07...</td>\n",
       "      <td>071390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            case_url   docket\n",
       "0  http://caselaw.findlaw.com/us-supreme-court/05...   051101\n",
       "1  http://caselaw.findlaw.com/us-supreme-court/06...  0611951\n",
       "2  http://caselaw.findlaw.com/us-supreme-court/06...    06263\n",
       "3  http://caselaw.findlaw.com/us-supreme-court/06...   065590\n",
       "4  http://caselaw.findlaw.com/us-supreme-court/07...   071390"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5) #Return the first n rows (default n=5) to check table values and header names aligned correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CindyBian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://caselaw.findlaw.com/us-supreme-court/05-1101.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ix[0, \"case_url\"] #select a specific row in the dataframe to check value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"supcourt_yearlist.pickle\") #Python Object serialization - “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23393, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #The shape attribute for numpy arrays returns the dimensions of the array\n",
    "\n",
    "#this will return the number of cases in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Scientific Packages into Python Kernel\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supcourt = pd.read_pickle(\"supcourt_yearlist.pickle\") #Read in output from previous step (Year and case title df from previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataframe into three temporary dfs (caselaw can detect too may requests from a scraper and block your ip)\n",
    "test_df = supcourt.iloc[5000:15000]\n",
    "test3_df = supcourt.iloc[0:5000]\n",
    "test2_df = supcourt.iloc[15000:23268]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function which collects the description of each case and appends to data table\n",
    "def supcourtdescr(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    allitems = []\n",
    "    response = requests.get(link, headers =  headers)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    pagesoup = soup.find_all(class_=\"caselawcontent searchable-content\") \n",
    "    \n",
    "    for item in pagesoup:\n",
    "        txtt = item.get_text()\n",
    "        allitems.append(txtt)\n",
    "    return ' '.join(allitems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CindyBian\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\CindyBian\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "test_df.loc[:,(\"case\")] = test_df.case_url.apply(supcourtdescr)\n",
    "test_df.to_pickle(\"temp2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2_df.loc[:,(\"case\")] = test2_df.case_url.apply(supcourtdescr)\n",
    "test2_df.to_pickle(\"temp1.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test3_df.loc[:,(\"case\")] = test3_df.case_url.apply(supcourtdescr)\n",
    "test3_df.to_pickle(\"temp3.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_project = pd.concat([test2_df, test3_df, test_df]) #putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_project.to_pickle(\"full_proj_preproc.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.corpus import names as names\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "male_names = [w.lower() for w in male_names]\n",
    "male_names_plur = [(w.lower() + \"s\") for w in male_names]\n",
    "female_names_plur = [(w.lower() + \"s\") for w in female_names]\n",
    "female_names = [w.lower() for w in female_names]\n",
    "casenames = list(pd.read_csv(\"casetitles.csv\",encoding = 'iso-8859-1'))\n",
    "statenames = list(pd.read_csv(\"statenames.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homespun_words = ['join', 'seek', 'ginnane', 'kestenbaum', 'hummel', 'loevinger', 'note', 'curiam', 'mosk', 'pd', \\\n",
    "                'paxton', 'rhino', 'buchsbaum', 'hirshowitz', 'misc', 'assistant', 'whereon', 'dismiss', 'sod', \\\n",
    "                'vote', 'present', 'entire', 'frankfurter', 'ante', 'leave', 'concur', 'entire', 'mootness', \\\n",
    "                'track', 'constitution', 'jj', 'blackmun', 'rehnquist', 'amici,sup', 'rep', 'stat', 'messes', \\\n",
    "                'like', 'rev', 'trans', 'bra', 'teller', 'vii', 'erisa', 'usca', 'annas', 'lead', 'cf', 'cca', \\\n",
    "                'fsupp', 'afdc', 'amicus', 'ante', 'orrick', 'kansa', 'pd', 'foth', 'stucky', 'aver',\"united\", \\\n",
    "                \"may\", \"argued\", \"argue\", \"decide\", \"rptr\", \"nervine\", \"pp\",\"fd\" ,\"june\", \"july\", \\\n",
    "                \"august\", \"september\", \"october\", \"november\", \"states\", \"ca\", \"joyce\", \"certiorari\", \"december\",\\\n",
    "                \"january\", \"february\", \"march\", \"april\", \"writ\", \"supreme court\", \"court\", \"dissent\", \\\n",
    "                \"opinion\", \"footnote\",\"brief\", \"decision\", \"member\", \"curiam\", \"dismiss\", \"note\", \"affirm\", \\\n",
    "                \"question\", \"usc\", \"file\"]\n",
    "\n",
    "STOPLIST = set(stopwords.words('english') + list(homespun_words) + list(ENGLISH_STOP_WORDS) \\\n",
    "               + list(statenames) + list(casenames) + list(female_names) + list(male_names) + \\ \n",
    "               list(female_names_plur) + list(male_names_plur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPLIST = set(list(stopwords.words('english')) + list(sub_list) + list(ENGLISH_STOP_WORDS))\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    separators = [\"\\xa0\\xa0\\xa0\\xa0\", \"\\r\", \"\\n\", \"\\t\", \"n't\", \"'m\", \"'ll\", '[^a-z ]']\n",
    "    for i in separators:\n",
    "        sample = re.sub(i, \" \", sample.lower())\n",
    "        \n",
    "    ## get the tokens using spaCy - this makes it possible to lemmatize the words\n",
    "    tokens = nlp(sample)\n",
    "    tokens = [tok.lemma_.strip() for tok in tokens]\n",
    "\n",
    "    ## apply our stoplist\n",
    "    return [tok for tok in tokens if len(tok) != 1 and tok not in STOPLIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_list[\"lem\"] = doc_list.case.apply(text_processing)\n",
    "doc_list.to_pickle(\"full_proj_lemmatized.pickle\") ## to be used in model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_list.read_pickle(\"full_proj_lemmatized3.pickle\") #always save your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_list.shape #checking to make sure we have the info we expected to have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def modeler(corp, n_topics, n_top_words, clf, vect):\n",
    "    df = .80\n",
    "    str_vect = str(vect).split(\"(\")[0]\n",
    "    str_clf = str(clf).split(\"(\")[0]\n",
    "\n",
    "    print(\"Extracting {} features for {}...\".format(str_vect, str_clf))\n",
    "    vect_trans = vect.fit_transform(corp)\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    print(\"Fitting the {} model with {} features, \"\n",
    "          \"n_topics= {}, n_topic_words= {}, n_features= {}...\"\n",
    "          .format(str_clf, str_vect, n_topics, n_top_words, n_features))\n",
    "\n",
    "    clf = clf.fit(vect_trans)\n",
    "    if str_clf == \"TruncatedSVD\":\n",
    "        print(\"\\nExplained variance ratio\", clf.explained_variance_ratio_)\n",
    "        \n",
    "    print(\"\\nTopics in {} model:\".format(str_clf))\n",
    "    feature_names = vect.get_feature_names()\n",
    "    return print_top_words(clf, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeler(doc_list.lem, 30, 30, LatentDirichletAllocation(n_topics=30, max_iter=5, learning_method='online', \\\n",
    "        learning_offset=50.,random_state=0), CountVectorizer(max_df=.80, min_df=2, \n",
    "                                                             stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_mod(doc_list.lem, .95, 2, 2000,10) #df is a way to extract 'meaningful text' in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeler(doc_list.lem, 100, 30, TruncatedSVD(2, algorithm = 'arpack'), TfidfVectorizer(max_df=.8, min_df=2,stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeler(doc_list.lem, 30, 30, NMF(n_components=30, random_state=1, alpha=.1, l1_ratio=.5), \\ \n",
    "        TfidfVectorizer(max_df=.98, min_df=2,stop_words='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################  modeling imports  #######################################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "#from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"full_proj_lemmatized3.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.ix[15000, \"case_url\"]\n",
    "#'http://caselaw.findlaw.com/us-supreme-court/382/12.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmf_mod(corp ):\n",
    "    df = .80\n",
    "    n_topics = 30\n",
    "    n_features = 2000\n",
    "    n_top_words = 40\n",
    "    \n",
    "    # Use tf-idf features for NMF.\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=df, min_df=5, # ngram_range=(1,2), #max_features=n_features,\n",
    "                                       stop_words='english')\n",
    "\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corp)\n",
    "\n",
    "\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model with tf-idf features, \"\n",
    "          \"n_topics= %d, n_topic_words= %d, n_features= %d...\"\n",
    "          % (n_topics, n_top_words, n_features))\n",
    "\n",
    "    nmf = NMF(n_components=n_topics, random_state=2, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "    \n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    #return print_top_words(nmf, tfidf_feature_names, n_top_words) \n",
    "    return tfidf,nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf, nmf_mod_test = nmf_mod(df.lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out =nmf_mod_test.transform(tfidf)\n",
    "out[49] #verified that each of these is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "topics = []\n",
    "for item in out:\n",
    "    max_index, max_value = max(enumerate(item), key=operator.itemgetter(1))\n",
    "    topics.append(max_index) \n",
    "    \n",
    "df[\"topicnumber\"] = pd.Series(topics, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_likelihood = []\n",
    "for item in out:\n",
    "    max_index, max_value = max(enumerate(item), key=operator.itemgetter(1))\n",
    "    topics_likelihood.append(max_value)\n",
    "    \n",
    "df[\"strengthoftopic\"] = pd.Series(topics_likelihood, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.topicnumber.value_counts() #let's make sure this is a good model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmf_topics_dict(corp, n_topics):\n",
    "    df = .80\n",
    "    n_top_words = 40\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=df, min_df=5,# ngram_range=(1,2), #max_features=n_features,\n",
    "                                       stop_words='english')\n",
    "\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corp)\n",
    "    nmf = NMF(n_components=n_topics, random_state=2, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "      \n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        topic_dict[topic_idx] = \", \".join([tfidf_feature_names[i] \\\n",
    "                                    for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After testing different topic distributions, 30 was optimal\n",
    "nmf_words_30 = nmf_topics_dict(df.lem, 30) #dict object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_words_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('finaliteration_topics.json', 'w') as fp:\n",
    "    json.dump(nmf_words_30, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_lookup(num):\n",
    "    return nmf_words_30.get(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"words\"] = df.topicnumber.apply(word_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.ix[15017,\"words\"] # This cell and the one below verifies that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.ix[14972,\"lem\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.ix[15017,\"case_url\"]\n",
    "# 'http://caselaw.findlaw.com/us-supreme-court/380/145.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"full_project_modelled_final.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"full_project_modelled_final.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some topics were extremely similar and at the suggestion of my instructors,\n",
    "# for the sake of the visualization, I have condensed the topics to 20\n",
    "\n",
    "def topic_condenser(topicnum):\n",
    "    if topicnum == 20:\n",
    "        return 24\n",
    "    if topicnum == 25:\n",
    "        return 1\n",
    "    if topicnum == 2:\n",
    "        return 12\n",
    "    if topicnum == 27:\n",
    "        return 26\n",
    "    if topicnum == 18 or topicnum == 5:\n",
    "        return 29\n",
    "    if topicnum == 8 or topicnum == 22:\n",
    "        return 7\n",
    "    if topicnum == 15:\n",
    "        return 16\n",
    "    if topicnum == 9:\n",
    "        return 14\n",
    "    if topicnum == 19:\n",
    "        return 3\n",
    "    else: \n",
    "        return topicnum\n",
    "df[\"condensedtopics\"] = df.topicnumber.apply(topic_condenser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doing some research on the not so obvious topics\n",
    "df = df[df[\"topicnumber\"] != 2]\n",
    "#df_16.ix[15065, \"caseurl\"]\n",
    "df_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_details = pd.read_csv(\"detailsford3.csv\", encoding = 'iso-8859-1')\n",
    "df_details.columns = [\"condensedtopics\", \"topicname\", \"title\", \"exampleURL\", \"leadpp\", \"topicwords\"]\n",
    "df_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_details = pd.merge(df, df_details, how = \"inner\", on = \"condensedtopics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#temp_df = df_with_details[['years', 'condensedtopics', \"topicname\", \"title\", \"exampleURL\", \"leadpp\", \"topicwords\"]]\n",
    "#temp_df.to_csv(\"temp.csv\")\n",
    "temp_df = df_with_details[['years', 'condensedtopics']]\n",
    "temp_df.condensedtopics.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dummy value for each existing topic. Pay no attention to this error.\n",
    "temp_df[\"count\"] = 1\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this condenses each point for the same year into n number of points \n",
    "temp_df = temp_df.groupby([\"years\", \"condensedtopics\"]).count().reset_index()\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_fillna = temp_df.pivot_table(\"count\", \"years\", \"condensedtopics\").fillna(0).unstack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we lose the count label column in the previous steps, so we're just renaming it here, and reordering columns based on \n",
    "#how they are arranged in the viz csv\n",
    "data_fillna.columns = [\"condensedtopics\", \"years\", \"count\"]\n",
    "data_fillna = data_fillna[[\"years\", \"condensedtopics\", \"count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge data\n",
    "final_data = pd.merge(data_fillna, df_details, how = \"inner\", on = \"condensedtopics\")\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sort by year\n",
    "final_data.sort_values(\"years\", inplace = True, ascending = True)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#backup file\n",
    "final_data.to_csv(\"topicsbyyear.csv\", index = False)\n",
    "final_data.to_csv(\"year_topic_data2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''the best part of this viz is the brushing side to side effect. For that, we need total cases for every year\n",
    "and need no other information'''\n",
    "\n",
    "data_fillna.groupby(\"years\")[\"count\"].sum().reset_index().to_csv(\"year_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
